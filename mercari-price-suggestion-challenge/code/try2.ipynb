{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T14:56:48.822462Z",
     "start_time": "2018-02-14T14:56:48.809973Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "import sys\n",
    "\n",
    "###Add https://www.kaggle.com/anttip/wordbatch to your kernel Data Sources,\n",
    "###until Kaggle admins fix the wordbatch pip package installation\n",
    "###sys.path.insert(0, '../input/wordbatch/wordbatch/')\n",
    "import wordbatch\n",
    "\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FTRL, FM_FTRL\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "NUM_BRANDS = 4500\n",
    "NUM_CATEGORIES = 1200\n",
    "\n",
    "# develop = False\n",
    "develop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic funcs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T14:56:51.093910Z",
     "start_time": "2018-02-14T14:56:51.050065Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(y0), 2)))\n",
    "\n",
    "\n",
    "def split_cat(text):\n",
    "    try:\n",
    "        return text.split(\"/\")\n",
    "    except:\n",
    "        return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['general_cat'].fillna(value='missing', inplace=True)\n",
    "    dataset['subcat_1'].fillna(value='missing', inplace=True)\n",
    "    dataset['subcat_2'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category1 = dataset['general_cat'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    pop_category2 = dataset['subcat_1'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    pop_category3 = dataset['subcat_2'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    dataset.loc[~dataset['general_cat'].isin(pop_category1), 'general_cat'] = 'missing'\n",
    "    dataset.loc[~dataset['subcat_1'].isin(pop_category2), 'subcat_1'] = 'missing'\n",
    "    dataset.loc[~dataset['subcat_2'].isin(pop_category3), 'subcat_2'] = 'missing'\n",
    "\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['general_cat'] = dataset['general_cat'].astype('category')\n",
    "    dataset['subcat_1'] = dataset['subcat_1'].astype('category')\n",
    "    dataset['subcat_2'] = dataset['subcat_2'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n",
    "\n",
    "\n",
    "# Define helpers for text normalization\n",
    "stopwords = {x: 1 for x in stopwords.words('english')}\n",
    "non_alphanums = re.compile(u'[^A-Za-z0-9]+')\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return u\" \".join(\n",
    "        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n",
    "         if len(x) > 1 and x not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T14:56:52.063362Z",
     "start_time": "2018-02-14T14:56:52.054453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'neon pink clear tech 21 iphone case used month shows major signs wear two exact ones available interested also two purple ones interested also let know'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(\"neon pink and clear tech 21 iphone 6 case! used for about a month, but shows no major signs of wear! **i have two of these exact ones available if interested** i also have two purple ones, if interested also just let me know!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features extract "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic fearture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T14:57:14.318314Z",
     "start_time": "2018-02-14T14:56:53.664022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-14 14:56:53\n",
      "[7.79165887833] Finished to load data\n",
      "('Train shape: ', (1482535, 8))\n",
      "('Test shape: ', (693359, 7))\n",
      "[15.6914129257] Split categories completed.\n",
      "[16.5526938438] Handle missing completed.\n",
      "[19.1612000465] Cut completed.\n",
      "[20.6273288727] Convert categorical completed\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "from time import gmtime, strftime\n",
    "print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
    "\n",
    "# if 1 == 1:\n",
    "###train = pd.read_table('../input/mercari-price-suggestion-challenge/train.tsv', engine='c')\n",
    "###test = pd.read_table('../input/mercari-price-suggestion-challenge/test.tsv', engine='c')\n",
    "\n",
    "train = pd.read_table('../input/train.tsv', engine='c')\n",
    "test = pd.read_table('../input/test.tsv', engine='c')\n",
    "\n",
    "print('[{}] Finished to load data'.format(time.time() - start_time))\n",
    "print('Train shape: ', train.shape)\n",
    "print('Test shape: ', test.shape)\n",
    "nrow_test = train.shape[0]  # -dftt.shape[0]\n",
    "dftt = train[(train.price < 1.0)]\n",
    "train = train.drop(train[(train.price < 1.0)].index)\n",
    "del dftt['price']\n",
    "nrow_train = train.shape[0]\n",
    "# print(nrow_train, nrow_test)\n",
    "y = np.log1p(train[\"price\"])\n",
    "merge = pd.concat([train, dftt, test])\n",
    "submission = test[['test_id']]\n",
    "\n",
    "del train\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "merge['general_cat'], merge['subcat_1'], merge['subcat_2'] = \\\n",
    "    zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n",
    "merge.drop('category_name', axis=1, inplace=True)\n",
    "print('[{}] Split categories completed.'.format(time.time() - start_time))\n",
    "\n",
    "handle_missing_inplace(merge)\n",
    "print('[{}] Handle missing completed.'.format(time.time() - start_time))\n",
    "\n",
    "cutting(merge)\n",
    "print('[{}] Cut completed.'.format(time.time() - start_time))\n",
    "\n",
    "to_categorical(merge)\n",
    "print('[{}] Convert categorical completed'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T14:57:40.290791Z",
     "start_time": "2018-02-14T14:57:40.262853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>test_id</th>\n",
       "      <th>train_id</th>\n",
       "      <th>general_cat</th>\n",
       "      <th>subcat_1</th>\n",
       "      <th>subcat_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>missing</td>\n",
       "      <td>3</td>\n",
       "      <td>No description yet</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Men</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-shirts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Razer</td>\n",
       "      <td>3</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Computers &amp; Tablets</td>\n",
       "      <td>Components &amp; Parts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Target</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Women</td>\n",
       "      <td>Tops &amp; Blouses</td>\n",
       "      <td>Blouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>New with tags. Leather horses. Retail for [rm]...</td>\n",
       "      <td>Leather Horse Statues</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Home</td>\n",
       "      <td>Home Décor</td>\n",
       "      <td>Home Décor Accents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>Complete with certificate of authenticity</td>\n",
       "      <td>24K GOLD plated rose</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Women</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>Necklaces</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  brand_name item_condition_id  \\\n",
       "0    missing                 3   \n",
       "1      Razer                 3   \n",
       "2     Target                 1   \n",
       "3    missing                 1   \n",
       "4    missing                 1   \n",
       "\n",
       "                                    item_description  \\\n",
       "0                                 No description yet   \n",
       "1  This keyboard is in great condition and works ...   \n",
       "2  Adorable top with a hint of lace and a key hol...   \n",
       "3  New with tags. Leather horses. Retail for [rm]...   \n",
       "4          Complete with certificate of authenticity   \n",
       "\n",
       "                                  name  price  shipping  test_id  train_id  \\\n",
       "0  MLB Cincinnati Reds T Shirt Size XL   10.0         1      NaN       0.0   \n",
       "1     Razer BlackWidow Chroma Keyboard   52.0         0      NaN       1.0   \n",
       "2                       AVA-VIV Blouse   10.0         1      NaN       2.0   \n",
       "3                Leather Horse Statues   35.0         1      NaN       3.0   \n",
       "4                 24K GOLD plated rose   44.0         0      NaN       4.0   \n",
       "\n",
       "   general_cat             subcat_1            subcat_2  \n",
       "0          Men                 Tops            T-shirts  \n",
       "1  Electronics  Computers & Tablets  Components & Parts  \n",
       "2        Women       Tops & Blouses              Blouse  \n",
       "3         Home           Home Décor  Home Décor Accents  \n",
       "4        Women              Jewelry           Necklaces  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nlp feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T15:09:56.048020Z",
     "start_time": "2018-02-14T15:09:55.983875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree  -  0.44350971281100476\n",
      "travellers  -  0.5174614751013837\n",
      "jupiter  -  0.5174614751013837\n",
      "fruit  -  0.5174614751013837\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7413c9bf3546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtop_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfidf_sorting\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_array' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer( stop_words='english')\n",
    "t=\"\"\"Two Travellers, walking in the noonday sun, sought the shade of a widespreading tree to rest. As they lay looking up among the pleasant leaves, they saw that it was a Plane Tree.\n",
    "\n",
    "\"How useless is the Plane!\" said one of them. \"It bears no fruit whatever, and only serves to litter the ground with leaves.\"\n",
    "\n",
    "\"Ungrateful creatures!\" said a voice from the Plane Tree. \"You lie here in my cooling shade, and yet you say I am useless! Thus ungratefully, O Jupiter, do men receive their blessings!\"\n",
    "\n",
    "Our best blessings are often the least appreciated.\"\"\"\n",
    "\n",
    "tfs = tfidf.fit_transform(t.split(\" \"))\n",
    "str = 'tree cat travellers fruit jupiter'\n",
    "response = tfidf.transform([str])\n",
    "feature_names = tfidf.get_feature_names()\n",
    "for col in response.nonzero()[1]:\n",
    "    print feature_names[col], ' - ', response[0, col]\n",
    "tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "\n",
    "n = 3\n",
    "top_n = feature_array[tfidf_sorting][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T15:08:39.616612Z",
     "start_time": "2018-02-14T15:07:35.132904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2175894x197497 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 31331442 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf.fit_transform(merge['item_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T05:55:39.505837Z",
     "start_time": "2018-02-12T05:52:28.126714Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordbags\n",
      "[178.317242146] Vectorize `name` completed.\n",
      "[212.42111516] Count vectorize `categories` completed.\n"
     ]
    }
   ],
   "source": [
    "wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0],\n",
    "                                                                  \"hash_size\": 2 ** 29, \"norm\": None, \"tf\": 'binary',\n",
    "                                                                  \"idf\": None,\n",
    "                                                                  }), procs=8)\n",
    "wb.dictionary_freeze= True\n",
    "X_name = wb.fit_transform(merge['name'])\n",
    "del(wb)\n",
    "X_name = X_name[:, np.array(np.clip(X_name.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n",
    "print('[{}] Vectorize `name` completed.'.format(time.time() - start_time))\n",
    "\n",
    "wb = CountVectorizer()\n",
    "X_category1 = wb.fit_transform(merge['general_cat'])\n",
    "X_category2 = wb.fit_transform(merge['subcat_1'])\n",
    "X_category3 = wb.fit_transform(merge['subcat_2'])\n",
    "print('[{}] Count vectorize `categories` completed.'.format(time.time() - start_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T06:01:51.483873Z",
     "start_time": "2018-02-12T05:55:39.510624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordbags\n",
      "[429.900854111] Vectorize `item_description` completed.\n",
      "[579.087362051] Label binarize `brand_name` completed.\n",
      "[584.397417068] Get dummies on `item_condition_id` and `shipping` completed.\n",
      "((2175894, 6), (2175894, 2040339), (2175894, 4501), (2175894, 14), (2175894, 143), (2175894, 977), (2175894, 518467))\n"
     ]
    }
   ],
   "source": [
    "# wb= wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 3, \"hash_ngrams_weights\": [1.0, 1.0, 0.5],\n",
    "# wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 3, \"hash_ngrams_weights\": [1.0, 1.0,1.0],\n",
    "#                                                               \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "#                                                               \"idf\": 50})\n",
    "#                          , procs=8)\n",
    "wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "                                                                  \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "                                                                  \"idf\": None})\n",
    "                         , procs=8)\n",
    "wb.dictionary_freeze= True\n",
    "X_description = wb.fit_transform(merge['item_description'])\n",
    "del(wb)\n",
    "X_description = X_description[:, np.array(np.clip(X_description.getnnz(axis=0) - 1, 0, 3), dtype=bool)]\n",
    "print('[{}] Vectorize `item_description` completed.'.format(time.time() - start_time))\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb.fit_transform(merge['brand_name'])\n",
    "print('[{}] Label binarize `brand_name` completed.'.format(time.time() - start_time))\n",
    "\n",
    "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n",
    "                                      sparse=True).values)\n",
    "print('[{}] Get dummies on `item_condition_id` and `shipping` completed.'.format(time.time() - start_time))\n",
    "print(X_dummies.shape, X_description.shape, X_brand.shape, X_category1.shape, X_category2.shape, X_category3.shape,\n",
    "      X_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T18:37:39.979954Z",
     "start_time": "2018-02-12T18:35:29.195986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordhashes\n",
      "(2175894, 33554432)\n",
      "(2175894, 1058234)\n",
      "[45826.9948261] Vectorize `name` completed.\n",
      "Normalize text\n",
      "Extract wordhashes\n",
      "(2175894, 33554432)\n",
      "(2175894, 6509517)\n"
     ]
    }
   ],
   "source": [
    "# wb_one = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 3, \"hash_ngrams_weights\": [1.0,1.0,1.0],\n",
    "#                                                               \"hash_size\": 2 ** 28, \"norm\": None, \"tf\": 'binary',\n",
    "#                                                               \"idf\": None,\n",
    "#                                                               }), procs=8)\n",
    "wb_one = wordbatch.WordBatch(normalize_text, extractor=(WordHash, {\"decode_error\":'ignore', \"n_features\":2 ** 25,\n",
    "                                             \"non_negative\":False, \"ngram_range\":(1,3), \"norm\":'l2'}), procs=8)\n",
    "\n",
    "wb_one.dictionary_freeze= True\n",
    "X_name_one = wb_one.fit_transform(merge['name'])\n",
    "print X_name_one.shape\n",
    "X_name_one = X_name_one[:, np.array(np.clip(X_name_one.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n",
    "print X_name_one.shape\n",
    "print('[{}] Vectorize `name` completed.'.format(time.time() - start_time))\n",
    "X_description_one = wb_one.fit_transform(merge['item_description'])\n",
    "del(wb_one)\n",
    "print X_description_one.shape\n",
    "X_description_one = X_description_one[:, np.array(np.clip(X_description_one.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n",
    "print X_description_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T18:38:29.419985Z",
     "start_time": "2018-02-12T18:37:39.988946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45982.333497] Label binarize `X_des_length` completed.\n"
     ]
    }
   ],
   "source": [
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_des_length = lb.fit_transform(merge['item_description'].apply(lambda x:len(normalize_text(x))/30))\n",
    "print('[{}] Label binarize `X_des_length` completed.'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T18:38:29.435009Z",
     "start_time": "2018-02-12T18:38:29.424932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2175894x1058234 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 17154208 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_name_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T18:40:42.761584Z",
     "start_time": "2018-02-12T18:38:29.439709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46015.333951] Create sparse merge completed\n",
      "(2175894, 10132231)\n",
      "[46115.6291699] Create sparse merge completed\n"
     ]
    }
   ],
   "source": [
    "sparse_merge = hstack((X_dummies, \n",
    "                       X_description,\n",
    "                       X_brand, X_category1, X_category2,\n",
    "                       X_category3, \n",
    "                       X_name,\n",
    "                       X_name_one,\n",
    "                       X_description_one,\n",
    "                       X_des_length\n",
    "                      )).tocsr()\n",
    "\n",
    "print('[{}] Create sparse merge completed'.format(time.time() - start_time))\n",
    "\n",
    "#    pd.to_pickle((sparse_merge, y), \"xy.pkl\")\n",
    "# else:\n",
    "#    nrow_train, nrow_test= 1481661, 1482535\n",
    "#    sparse_merge, y = pd.read_pickle(\"xy.pkl\")\n",
    "\n",
    "# Remove features with document frequency <=1\n",
    "print(sparse_merge.shape)\n",
    "mask = np.array(np.clip(sparse_merge.getnnz(axis=0) - 1, 0, 3), dtype=bool)\n",
    "sparse_merge = sparse_merge[:, mask]\n",
    "print('[{}] Create sparse merge completed'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T18:40:42.813418Z",
     "start_time": "2018-02-12T18:40:42.789164Z"
    }
   },
   "outputs": [],
   "source": [
    "#one word for name ;one word for item desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T18:40:42.854385Z",
     "start_time": "2018-02-12T18:40:42.820050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sparse_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T18:42:31.011775Z",
     "start_time": "2018-02-12T18:40:42.860445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2175894, 10131715)\n",
      "(2175894, 10131715)\n",
      "(148166, 10131715) (37042, 10131715) (148166,) (37042,)\n"
     ]
    }
   ],
   "source": [
    "print(sparse_merge.shape)\n",
    "mask = np.array(np.clip(sparse_merge.getnnz(axis=0) - 1, 0, 1), dtype=bool)\n",
    "sparse_merge = sparse_merge[:, mask]\n",
    "X = sparse_merge[:nrow_train]\n",
    "X_test = sparse_merge[nrow_test:]\n",
    "print(sparse_merge.shape)\n",
    "\n",
    "gc.collect()\n",
    "train_X, train_y = X, y\n",
    "if develop:\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(X, y, train_size =0.1,test_size=0.025, random_state=100)\n",
    "    print train_X.shape,valid_X.shape,train_y.shape,valid_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ftrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-12T18:35:33.028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46256.897579] Train FTRL completed\n",
      "('FTRL dev RMSLE:', 0.47863714593804524)\n",
      "[46260.970403] Predict FTRL completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=sparse_merge.shape[1], iters=30, inv_link=\"identity\", threads=1)\n",
    "\n",
    "model.fit(train_X, train_y)\n",
    "print('[{}] Train FTRL completed'.format(time.time() - start_time))\n",
    "if develop:\n",
    "    preds = model.predict(X=valid_X)\n",
    "    print(\"FTRL dev RMSLE:\",rmsle(np.expm1(valid_y), np.expm1(preds)))\n",
    "\n",
    "predsF = model.predict(X_test)\n",
    "print('[{}] Predict FTRL completed'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fm_ftrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-12T18:35:34.546Z"
    }
   },
   "outputs": [],
   "source": [
    "model = FM_FTRL(alpha=0.03, beta=0.01, L1=0.00001, L2=0.1, D=sparse_merge.shape[1], alpha_fm=0.01, L2_fm=0.0, init_fm=0.01,\n",
    "                    D_fm=200, e_noise=0.0001, iters=15, inv_link=\"identity\", threads=4)\n",
    "\n",
    "model.fit(train_X, train_y)\n",
    "print('[{}] Train ridge v2 completed'.format(time.time() - start_time))\n",
    "if develop:\n",
    "    preds = model.predict(X=valid_X)\n",
    "    print(\"FM_FTRL dev RMSLE:\", rmsle(np.expm1(valid_y), np.expm1(preds)))\n",
    "\n",
    "predsFM = model.predict(X_test)\n",
    "print('[{}] Predict FM_FTRL completed'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T07:27:31.016880Z",
     "start_time": "2018-02-11T07:27:30.795449Z"
    }
   },
   "source": [
    "### lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T07:06:12.651347Z",
     "start_time": "2018-02-12T07:04:42.047633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[30]\ttraining's rmse: 0.55081\tvalid_1's rmse: 0.558343\n",
      "[60]\ttraining's rmse: 0.520466\tvalid_1's rmse: 0.533792\n",
      "[90]\ttraining's rmse: 0.503777\tvalid_1's rmse: 0.522356\n",
      "[120]\ttraining's rmse: 0.492175\tvalid_1's rmse: 0.515612\n",
      "[150]\ttraining's rmse: 0.483006\tvalid_1's rmse: 0.510733\n",
      "[180]\ttraining's rmse: 0.474618\tvalid_1's rmse: 0.506801\n",
      "[210]\ttraining's rmse: 0.468332\tvalid_1's rmse: 0.50381\n",
      "[240]\ttraining's rmse: 0.462052\tvalid_1's rmse: 0.501793\n",
      "[270]\ttraining's rmse: 0.456878\tvalid_1's rmse: 0.500006\n",
      "[300]\ttraining's rmse: 0.451802\tvalid_1's rmse: 0.498694\n",
      "[330]\ttraining's rmse: 0.446665\tvalid_1's rmse: 0.496706\n",
      "[360]\ttraining's rmse: 0.442946\tvalid_1's rmse: 0.495952\n",
      "[390]\ttraining's rmse: 0.438668\tvalid_1's rmse: 0.494638\n",
      "[420]\ttraining's rmse: 0.435109\tvalid_1's rmse: 0.493974\n",
      "[450]\ttraining's rmse: 0.432134\tvalid_1's rmse: 0.493154\n",
      "[480]\ttraining's rmse: 0.429051\tvalid_1's rmse: 0.492556\n",
      "('LGB dev RMSLE:', 0.49213062261739343)\n",
      "[4445.56351495] Predict LGB completed.\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        'learning_rate': 0.3,\n",
    "        'application': 'regression',\n",
    "        'max_depth': 10,\n",
    "        'num_leaves': 30,\n",
    "        'verbosity': -1,\n",
    "        'metric': 'RMSE',\n",
    "        'data_random_seed': 1,\n",
    "#         'bagging_fraction': 0.8,\n",
    "#         'bagging_freq': 5,\n",
    "        'feature_fraction': 0.9,\n",
    "        'nthread': 4,\n",
    "        'min_data_in_leaf': 20,\n",
    "#         'max_bin': 10000\n",
    "    }\n",
    "\n",
    "# # Remove features with document frequency <=100\n",
    "# print(sparse_merge.shape)\n",
    "# mask = np.array(np.clip(sparse_merge.getnnz(axis=0) - 100, 0, 1), dtype=bool)\n",
    "# sparse_merge = sparse_merge[:, mask]\n",
    "# X = sparse_merge[:nrow_train]\n",
    "# X_test = sparse_merge[nrow_test:]\n",
    "# print(sparse_merge.shape)\n",
    "\n",
    "# train_X, train_y = X, y\n",
    "# if develop:\n",
    "#     train_X, valid_X, train_y, valid_y = train_test_split(X, y,train_size=0.2,test_size=0.05, random_state=100)\n",
    "\n",
    "d_train = lgb.Dataset(train_X, label=train_y)\n",
    "watchlist = [d_train]\n",
    "if develop:\n",
    "    d_valid = lgb.Dataset(valid_X, label=valid_y)\n",
    "    watchlist = [d_train, d_valid]\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=500, valid_sets=watchlist, \\\n",
    "                  early_stopping_rounds=100, verbose_eval=30)\n",
    "\n",
    "if develop:\n",
    "    preds = model.predict(valid_X)\n",
    "    print(\"LGB dev RMSLE:\", rmsle(np.expm1(valid_y), np.expm1(preds)))\n",
    "\n",
    "predsL = model.predict(X_test)\n",
    "\n",
    "print('[{}] Predict LGB completed.'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": true
   },
   "outputs": [],
   "source": [
    "0.4560898110533874"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (predsF * 0.2 + predsL * 0.3 + predsFM * 0.5)\n",
    "\n",
    "submission['price'] = np.expm1(preds)\n",
    "submission.to_csv(\"submission_wordbatch_ftrl_fm_lgb.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "625px",
    "left": "0px",
    "right": "1217px",
    "top": "111px",
    "width": "191px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
